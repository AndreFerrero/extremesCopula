---
title: "Joint Copula Bayesian Modelling"
output:
  html_document:
    toc: true
    toc_float: true
---


# Motivation

We are assuming the following joint distribution for a sequence of random variables $(X_1, \dots, X_n) \sim C_\theta(F(X_1), \dots, F(X_n))$, with C a parametric copula (potentially archimedean) and F the marginal of each $X_i$.

We know from simulation results that the likelihood peaks at the correct copula generating parameter if we take the realisations $U_1, \dots, U_n \sim C_\theta$. However, estimating the margin F from a realisation of $X_1, \dots, X_n$, to obtain the pseudo observations $\hat{U}_i = \hat{F}(X_i)$, leads to a likelihood which peaks at $\theta = 1$, the independence case in the Gumbel copula. This phenomenon could be explained by realising that we are using estimators for the margin which work under independence, but might fail under the dependence.

```{r lik plot sim, echo = F, cache = TRUE, message=F}
set.seed(123)
library(copula)
# -----------------------------------------------------------
# 1. TRUE parameter and dimension
# -----------------------------------------------------------
n <- 100
theta_true <- 3

# -----------------------------------------------------------
# 2. Simulate ONE 100-dimensional vector from Gumbel copula
# -----------------------------------------------------------
cop <- gumbelCopula(param = theta_true, dim = n)
U <- as.numeric(rCopula(1, cop))   # true uniforms

# -----------------------------------------------------------
# 3. Generate X via lognormal margins
# -----------------------------------------------------------
mu <- 0
sigma <- 1
X <- qlnorm(U, meanlog = mu, sdlog = sigma)

# -----------------------------------------------------------
# 4A. Parametric margins
# -----------------------------------------------------------
mu_hat <- mean(log(X))
sigma_hat <- sd(log(X))
u_hat_param <- plnorm(X, meanlog = mu_hat, sdlog = sigma_hat)

# -----------------------------------------------------------
# 4B. ECDF pseudo-observations
# -----------------------------------------------------------
u_hat_ecdf <- rank(X) / (n + 1)

# -----------------------------------------------------------
# 5. Gumbel log-likelihood
# -----------------------------------------------------------
loglik_gumbel <- function(theta, u) {
  if (theta <= 1) return(-1e10)   # keep optimizer stable
  cop <- gumbelCopula(param = theta, dim = length(u)) 
  ll <- log(dCopula(u, copula = cop))
  return(ll)
}

# Negative log-likelihood for optimization
negLL <- function(theta, u) -loglik_gumbel(theta, u)

# -----------------------------------------------------------
# 6. Compute estimators via optim()
# -----------------------------------------------------------

# TRUE UNIFORMS
est_trueU <- optim(par = 5, fn = negLL, u = U, method = "L-BFGS-B",
                   lower = 1.001, upper = 30)$par

# PARAMETRIC MARGINS
est_param <- optim(par = 5, fn = negLL, u = u_hat_param, method = "L-BFGS-B",
                   lower = 1.001, upper = 30)$par

# ECDF PSEUDO-OBSERVATIONS
est_ecdf <- optim(par = 5, fn = negLL, u = u_hat_ecdf, method = "L-BFGS-B",
                   lower = 1.001, upper = 30)$par

# -----------------------------------------------------------
# 7. Likelihood curves for comparison
# -----------------------------------------------------------
theta_grid <- seq(1.01, 12, length.out = 200)

ll_trueU  <- sapply(theta_grid, loglik_gumbel, u = U)
ll_param  <- sapply(theta_grid, loglik_gumbel, u = u_hat_param)
ll_ecdf   <- sapply(theta_grid, loglik_gumbel, u = u_hat_ecdf)

```

```{r lik plot, echo = FALSE}
plot(theta_grid, ll_trueU, type="l", lwd=3, col="black",
     xlab=expression(theta), ylab="Log-likelihood",
     main = "Gumbel log-likelihood")

lines(theta_grid, ll_param, col="blue", lwd=2)
lines(theta_grid, ll_ecdf,  col="red",  lwd=2)

abline(v = theta_true, col="darkgreen", lty=2, lwd=2)

legend("topright",
       legend = c("True U", "Parametric Margin", "ECDF", "True theta"),
       col    = c("black", "blue", "red", "darkgreen"),
       lty    = c(1,1,1,2),
       lwd    = c(3,2,2,2))
       
```

The idea is to avoid the 2 steps procedure by performing estimation on the margin and the copula parameter simultaneously. A fully bayesian model would inherently consider both objects as targets, and might be able to consider the dependence structure while estimating the margin.

# Bayesian Model with Parametric Margin

The simplest approach consists in assuming a parametric form for the margin, that is $X_i \sim F_\varphi$, with $\varphi$ a generic parameter vector. Hierarchically, this yields the following structure:

$$
\begin{aligned}
\varphi \sim \pi_\varphi \, &, \quad \theta \sim \pi_\theta \\
(X_1, \dots, X_n) | \; \theta &, \varphi \sim C_\theta (F_\varphi (X_1), \dots, F_\varphi (X_n))
\end{aligned}
$$

## Posterior sampling with MCMC

We aim to sample from the joint log posterior:

$$
\log \pi(\theta,\varphi \mid x_{1:n})
\;\propto\;
\log \pi_\theta(\theta) +
\log \pi_\varphi(\varphi) +
\log c_\theta\!\left(F_\varphi(x_1), \dots, F_\varphi(x_n)\right) + 
\sum_{i=1}^n \log f_\varphi(x_i).
$$

We will call the full parameter vector:
$$
\boldsymbol{\phi} = (\theta,\varphi)\in\mathbb{R}^d.
$$

To sample from this posterior, a simple algorithm is a Random Walk Metropolisâ€“Hastings. Crucial is the proposal covariance $\Sigma_0$ set at the beginning. This determines the efficiency of the sampler while exploring the posterior distribution. Higher variance in the proposals implies the sampler will take with higher probability further steps, moving quicker around the posterior space but with a higher probability of ending up in regions of low posterior probability, increasing therefore the risk of rejection. In contrast, smaller proposal variance induces the sampler to take shorter steps, decreasing the risk of being rejected but at the same time exploring the posterior space more slowly. A good balance needs to be found and no simple solution exists.

\bigskip
\noindent\textbf{Algorithm 1: Metropolis--Hastings}

\begin{enumerate}
\item \textbf{Initialisation:}
  \begin{itemize}
    \item Set the initial parameter vector 
    \(\boldsymbol{\phi}^{(0)}\).
    \item Compute the initial log-posterior 
    \(\ell_{\text{curr}} = \ell(\boldsymbol{\phi}^{(0)})\).
    \item Set the proposal covariance  
      \(\Sigma_0 = \varepsilon_0 I_d\).
  \end{itemize}

\item \textbf{For} \(t = 1, \dots, N\):
  \begin{enumerate}
    \item \textbf{Proposal:}\\
    Draw
    \[
    \boldsymbol{\phi}^\star \sim 
      \mathcal{N}_d\!\left(\boldsymbol{\phi}^{(t-1)},\, \Sigma_0\right).
    \]

    \item \textbf{Evaluate proposed log-posterior:}\\
    Compute 
    \(\ell^\star = \ell(\boldsymbol{\phi}^\star)\).

    \item \textbf{Acceptance probability:}\\
    \[
    \alpha = 
    \min\{ 1,\,
      \exp(\ell^\star - \ell_{\text{curr}})
    \}.
    \]

    \item \textbf{Accept/reject:}\\
    Draw \(u\sim\mathrm{Unif}(0,1)\).
    \begin{itemize}
      \item If \(u < \alpha\): set  
        \(\boldsymbol{\phi}^{(t)} = \boldsymbol{\phi}^\star\)  
        and \(\ell_{\text{curr}} = \ell^\star\).
      \item Else: set  
        \(\boldsymbol{\phi}^{(t)} = \boldsymbol{\phi}^{(t-1)}\).
    \end{itemize}
  \end{enumerate}
\end{enumerate}

## Adaptive algorithms

To overcome the choice of the proposal covariance $\Sigma_0$, adaptive methods have been proposed. The idea behind adaptive MCMC is to learn the optimal proposal variance while the chain is running, or to target a specific acceptance rate considered to be optimal. In the case of multidimensional posterior, a standard acceptance rate to target is $\alpha^* = 0.234$.

We considered mainly two adaptive schemes: the Haario scheme and the Robbins-Monroe scheme.

The first one simply starts by an arbitrary covariance and updates it by computing the empirical covariance $\hat{\Sigma}$ of the chains at that moment. Instead, Robbins-Monroe scheme targets a specific acceptance rate $\alpha^*$ by updating the scale of the covariance iteratively.

Care must be taken when using adaptive methods as theoretical guarantees of ergodicity require some assumptions to be met, one of which is for example diminishing adaptation, to ensure that after a while adaptation practically stops and we ensure the Markov Chain will converge to its stationary distribution.

## Joint Copula Likelihood Intractability

The loglikelihood of our model is intractable due to the log density of the copula. After a certain sample size, this quantity cannot be easily computed, making inference impossible even for a moderately big sample.

```{r, error=TRUE}
library(copula)
set.seed(123)

n <- 1000
theta_true <- 2

cop <- gumbelCopula(param = theta_true, dim = n)
U <- as.numeric(rCopula(1, cop))   # true uniforms

dCopula(U, gumbelCopula(theta_true, dim = n))
```

## Simulation Based Inference

This branch of Bayesian inference overcomes the problem of intractable likelihood that makes posterior sampling otherwise impossible. Indeed, to compute $\alpha$ the likelihood needs to be evaluated for the proposed parameter $\phi^*$.

The main idea is to (efficiently) sample from the model multiple times and to summarise the obtained samples with summary statistics that are informative about the parameters of interest. With the Bayesian Synthetic Likelihood (BSL) approach, the joint distribution of the summary statistics is used to approximate the likelihood for the proposed parameter vector.

