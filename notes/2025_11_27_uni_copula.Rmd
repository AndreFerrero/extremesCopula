---
title: "Copula Estimation - Univariate Case"
date: "2025-11-27"
output: pdf_document
---

# 1. Sampling with copula package

The copula package automatically samples bivariate and multivariate copulas with a convenient function. To use it in our univariate scenario, we conside the number of observations in the dataset as the dimension of the copula. Below the Gumbel case:

```{r}
set.seed(46)
library(copula)
theta <- 1.5
n <- 200

Gcop <- gumbelCopula(param = theta, dim = n)

U_cop <- rCopula(1, Gcop)
```

Choosing the margin allows to construct a sample $X_1, ..., X_n \sim H = C(F(x_1), ..., F(x_n))$ by applying $X_i = F^{-1} (U_i)$. For example, taking $X_i \sim Frechet(2)$:

```{r}
library(evd)
alpha <- 2
X_cop <- qfrechet(U_cop, shape = alpha)
```

```{r, echo = FALSE}
par(mfrow = c(1,2))
hist(U_cop, main = "Gumbel - copula package")
plot(density(X_cop), main = "Density of Gumbel-Frechet")
par(mfrow = c(1,1))
```

# 2. Stochastic Representation with latent V

To sample from the same copula, we can use the latent variable representation. The latent variable $V$ whose Laplace transform is the Gumbel generator $\psi(t) = \exp \{- t ^{\frac{1}{\theta}}\}$ has distribution $F_V \sim Stable(\alpha = 1/\theta, \beta = 1, \gamma = (cos(\frac{\pi}{2 \theta}))^\theta, \delta = 0 ; pm = 1)$.

```{r}
set.seed(46)
library(stabledist)

gum_psi <- function(t, theta){
    exp(- t ^ (1/theta))
}

## Stable parameters for gumbel

V <- rstable(
    n = 1,
    alpha = 1/theta,
    beta = 1,
    gamma = cospi(1/(2 * theta))^theta,
    delta = 0,
    pm = 1
)

E <- rexp(n, V) 

U_v <- gum_psi(E, theta)

X_v <- qfrechet(U_v, alpha)

```

```{r, echo = FALSE}
par(mfrow = c(1,2))
hist(U_v, main = "Gumbel - Latent Variable")
plot(density(X_v), main = "Density of Gumbel-Frechet")
par(mfrow = c(1,1))
```

For convenience, we write the sampler function for the U values (the quantile function is left unspecified to allow the freedom to choose the margin F):

```{r}
# Latent variable
rGumbV <- function(n, theta) {
    require(stabledist)

    # Gumbel V r.v.
    V <- rstable(
        n = 1,
        alpha = 1/theta,
        beta = 1,
        gamma = cospi(1/(2 * theta))^theta,
        delta = 0,
        pm = 1
    )

    E <- rexp(n, V) 

    # Gumbel generator
    gum_psi <- function(t, theta){
        exp(- t ^ (1/theta))
    }

    #(U_1, ..., U_n) ~ C_psi
    U <- gum_psi(E, theta)

    return(U)
}

```


# 3. Model likelihood and identifiability

# 3.1 Simulation

Below we can see different realisations from a Gumbel-Frechet model and different theta parameters. It seems that different theta parameters lead to very similar realisations. When we perform estimation of the parameter, could this create non identifiability?

```{r, echo = FALSE, cache = TRUE}
library(ggplot2)
library(dplyr)
library(copula)
library(evd)     # or whichever package provides qfrechet()

set.seed(1234)

# Parameters
thetas <- c(1.5, 2, 2.5, 3)
n <- 1000
n_sims <- 30
shape_frechet <- 2  # as requested

# Simulate U as before
all_data_U <- lapply(thetas, function(th) {
  
  latent_list <- lapply(1:n_sims, function(sim_id) {
    data.frame(
      U = rGumbV(n, th),
      theta = th,
      method = "Latent Variable",
      sim_id = sim_id
    )
  })
  
  Gcop <- gumbelCopula(param = th, dim = n)
  copula_list <- lapply(1:n_sims, function(sim_id) {
    data.frame(
      U = as.vector(rCopula(1, Gcop)),
      theta = th,
      method = "Copula Package",
      sim_id = sim_id
    )
  })
  
  do.call(rbind, c(latent_list, copula_list))
}) %>% bind_rows()

# Transform to Frechet(2) using quantile function
all_data_F <- all_data_U %>%
  mutate(
    X = qfrechet(U, shape = shape_frechet), 
    # If qfrechet requires also loc/scale arguments, add them (e.g. loc=0, scale=1)
    theta = theta,
    method = method,
    sim_id = sim_id
  )

# Plot 1: the U‑densities (as you had)
p_U <- ggplot(all_data_U, aes(x = U, group = sim_id, color = factor(sim_id))) +
  geom_density(alpha = 0.7, size = 0.4) +
  facet_grid(theta ~ method) +
  coord_cartesian(ylim = c(0, 5)) +
  labs(x = "U", y = "Density", title = paste0("Gumbel – ", n_sims, " replicates")) +
  theme_minimal(base_size = 14) +
  theme(legend.position = "none")

print(p_U)

# Plot 2: the Frechet(2) densities
p_F <- ggplot(all_data_F, aes(x = X, group = sim_id, color = factor(sim_id))) +
  geom_density(alpha = 0.7, size = 0.3) +
  facet_grid(theta ~ method, scales = "free_x") +
  coord_cartesian(ylim = c(0, 2), xlim = c(0,10)) +
  labs(x = "X", y = "Density",
       title = paste0("GFrechet - ", n_sims, " replicates")) +
  theme_minimal(base_size = 14) +
  theme(legend.position = "none")

print(p_F)

```

```{r, echo = FALSE, cache = TRUE}
library(ggplot2)
library(dplyr)
library(tidyr)
library(copula)
library(evd)

set.seed(1234)

# Parameters
thetas <- c(1.5, 2, 2.5, 3)
n <- 1000
n_sims <- 1000
shape_frechet <- 2

# Simulate U
all_data_U <- lapply(thetas, function(th) {
  latent_list <- lapply(1:n_sims, function(sim_id) {
    data.frame(
      U = rGumbV(n, th),
      theta = th,
      method = "Latent Variable",
      sim_id = sim_id
    )
  })
  
  Gcop <- gumbelCopula(param = th, dim = n)
  copula_list <- lapply(1:n_sims, function(sim_id) {
    data.frame(
      U = as.vector(rCopula(1, Gcop)),
      theta = th,
      method = "Copula Package",
      sim_id = sim_id
    )
  })
  
  do.call(rbind, c(latent_list, copula_list))
}) %>% bind_rows()

# Transform to Frechet(2)
all_data_F <- all_data_U %>%
  mutate(X = qfrechet(U, shape = shape_frechet))

# -----------------------------
# Function to compute median, mean, 95% CI for densities
# -----------------------------
compute_density_summary <- function(df, value_col, from = 0, to = NULL, n = 512) {
  df_summary <- df %>%
    group_by(theta, method, sim_id) %>%
    summarise(
      dens_obj = list(density(get(value_col), from = from, to = to, n = n)),
      .groups = "drop"
    ) %>%
    # Extract x and y from density object
    rowwise() %>%
    mutate(
      x = list(dens_obj$x),
      y = list(dens_obj$y)
    ) %>%
    select(-dens_obj) %>%
    unnest(c(x, y))  # unnest lists into long format

  # Compute median, mean, 95% CI across simulations
  df_stats <- df_summary %>%
    group_by(theta, method, x) %>%
    summarise(
      median_y = median(y),
      mean_y = mean(y),
      lower = quantile(y, 0.1),
      upper = quantile(y, 0.9),
      .groups = "drop"
    )
  
  return(df_stats)
}


# Compute summaries
U_summary <- compute_density_summary(all_data_U, "U", from = 0, to = 1)
F_summary <- compute_density_summary(all_data_F, "X", from = 0, to = 10)

# -----------------------------
# Plot U summary
# -----------------------------
# Plot U summary
p_U_summary <- ggplot(U_summary, aes(x = x)) +
  geom_ribbon(aes(ymin = lower, ymax = upper), fill = "lightblue", alpha = 0.4) +
  geom_line(aes(y = median_y), color = "blue", size = 1) +
  coord_cartesian(ylim = c(0, 4)) +
  facet_grid(theta ~ method) +
  labs(x = "U", y = "Density",
       title = paste0("Gumbel - ", n_sims, " replicates - Median and 80%")) +
  theme_minimal(base_size = 14)

print(p_U_summary)

# Plot Fréchet(2) summary
p_F_summary <- ggplot(F_summary, aes(x = x)) +
  geom_ribbon(aes(ymin = lower, ymax = upper), fill = "lightgreen", alpha = 0.4) +
  geom_line(aes(y = median_y), color = "green", size = 1) +
  coord_cartesian(ylim = c(0, 3)) +
  facet_grid(theta ~ method, scales = "free_x") +
  labs(x = "X", y = "Density",
       title = paste0("GFrechet - ", n_sims, " replicates - Median and 80%")) +
  theme_minimal(base_size = 14)

print(p_F_summary)
```

# 3.2 Likelihood

The aim is to show what is the shape of the likelihood of the model.

\[
\text{Model:}\qquad
(X_1,\dots,X_n)\sim H_n(\cdot)
\qquad\text{with}\qquad
H_n(x_1,\dots,x_n)
= C_{\theta}\!\big(F(x_1),\dots,F(x_n)\big),
\]
where $F$ is the marginal CDF, $f$ its density, and $C_{\theta}$ a copula (with copula density $c_{\theta}$).

\medskip

\begin{align}
&\; H_n(x_1,\dots,x_n)
= C_{\theta}\big(u_1,\dots,u_n\big),
\qquad u_i := F(x_i)
\nonumber\\
\Longrightarrow\quad
&\; h_n(x_1,\dots,x_n)
\;=\;
\frac{\partial^n}{\partial x_1\cdots\partial x_n}
H_n(x_1,\dots,x_n)
\nonumber\\
\overset{(\text{chain rule})}{=}\quad
&\;
\frac{\partial^n}{\partial u_1\cdots\partial u_n}
C_{\theta}(u_1,\dots,u_n)
\cdot
\prod_{i=1}^n \frac{\partial u_i}{\partial x_i}
\nonumber\\
\Rightarrow\quad
&\;
\boxed{\,h_n(x_1,\dots,x_n)
=
c_{\theta}\!\big(F(x_1),\dots,F(x_n)\big)\,
\prod_{i=1}^n f(x_i)\,}
\label{eq:joint-density}
\end{align}

\medskip

Now the likelihood (viewed as a function of the copula parameter $\theta$)
for the observed vector $(x_1,\dots,x_n)$ is
\begin{align}
L(\theta)
&\;=\; h_n(x_1,\dots,x_n;\theta)
\;=\;
c_{\theta}\big(F(x_1),\dots,F(x_n)\big)\,
\prod_{i=1}^n f(x_i).
\end{align}
Taking logarithms yields the log-likelihood
\begin{align}
\ell(\theta)
:= \log L(\theta)
&\;=\;
\log c_{\theta}\big(F(x_1),\dots,F(x_n)\big)
\;+\;
\sum_{i=1}^n \log f(x_i).
\label{eq:loglik}
\end{align}

Note that in the case of estimating only $\theta$, the log-likelihood is constant in the second term w.r.t. the parameter.
Note as well that for a too large sample size, the evaluation of the copula density is not feasible.

```{r, error = TRUE}
cop <- gumbelCopula(param = theta_true, dim = 1000)
U <- as.numeric(rCopula(1, cop))
dCopula(U, copula = cop)
```

To illustrate what happens to the MLE, we can compute the profile log-likelihood for a given sample with the following implementation. A crucial factor seems to be the estimation of the margin to obtain the U-values used to estimate the paremeter from the log-likelihood.

```{r, cache = TRUE}
set.seed(123)

# -----------------------------------------------------------
# 1. TRUE parameter and dimension
# -----------------------------------------------------------
n <- 100
theta_true <- 3

# -----------------------------------------------------------
# 2. Simulate ONE 100-dimensional vector from Gumbel copula
# -----------------------------------------------------------
cop <- gumbelCopula(param = theta_true, dim = n)
U <- as.numeric(rCopula(1, cop))   # true uniforms

# -----------------------------------------------------------
# 3. Generate X via lognormal margins
# -----------------------------------------------------------
mu <- 0
sigma <- 1
X <- qlnorm(U, meanlog = mu, sdlog = sigma)

# -----------------------------------------------------------
# 4A. Parametric margins
# -----------------------------------------------------------
mu_hat <- mean(log(X))
sigma_hat <- sd(log(X))
u_hat_param <- plnorm(X, meanlog = mu_hat, sdlog = sigma_hat)

# -----------------------------------------------------------
# 4B. ECDF pseudo-observations
# -----------------------------------------------------------
u_hat_ecdf <- rank(X) / (n + 1)

# -----------------------------------------------------------
# 5. Gumbel log-likelihood
# -----------------------------------------------------------
loglik_gumbel <- function(theta, u) {
  if (theta <= 1) return(-1e10)   # keep optimizer stable
  cop <- gumbelCopula(param = theta, dim = length(u)) 
  ll <- log(dCopula(u, copula = cop))
  return(ll)
}

# Negative log-likelihood for optimization
negLL <- function(theta, u) -loglik_gumbel(theta, u)

# -----------------------------------------------------------
# 6. Compute estimators via optim()
# -----------------------------------------------------------

# TRUE UNIFORMS
est_trueU <- optim(par = 5, fn = negLL, u = U, method = "L-BFGS-B",
                   lower = 1.001, upper = 30)$par

# PARAMETRIC MARGINS
est_param <- optim(par = 5, fn = negLL, u = u_hat_param, method = "L-BFGS-B",
                   lower = 1.001, upper = 30)$par

# ECDF PSEUDO-OBSERVATIONS
est_ecdf <- optim(par = 5, fn = negLL, u = u_hat_ecdf, method = "L-BFGS-B",
                   lower = 1.001, upper = 30)$par

# -----------------------------------------------------------
# 7. Likelihood curves for comparison
# -----------------------------------------------------------
theta_grid <- seq(1.01, 12, length.out = 200)

ll_trueU  <- sapply(theta_grid, loglik_gumbel, u = U)
ll_param  <- sapply(theta_grid, loglik_gumbel, u = u_hat_param)
ll_ecdf   <- sapply(theta_grid, loglik_gumbel, u = u_hat_ecdf)

```

```{r, echo = FALSE}
plot(theta_grid, ll_trueU, type="l", lwd=3, col="black",
     xlab=expression(theta), ylab="Log-likelihood",
     main = "Gumbel log-likelihood shapes")

lines(theta_grid, ll_param, col="blue", lwd=2)
lines(theta_grid, ll_ecdf,  col="red",  lwd=2)

abline(v = theta_true, col="darkgreen", lty=2, lwd=2)
abline(v = est_trueU,   col="black",    lty=3, lwd=2)
abline(v = est_param,   col="blue",     lty=3, lwd=2)
abline(v = est_ecdf,    col="red",      lty=3, lwd=2)

legend("topright",
       legend = c("True U", "Parametric Margin", "ECDF", 
                  "True theta", "theta_trueU"),
       col    = c("black", "blue", "red", "darkgreen", "black"),
       lty    = c(1,1,1,2,3),
       lwd    = c(3,2,2,2,2))
       
```

```{r, echo = FALSE}
cat("\nESTIMATED THETAS:\n")
cat("theta_true =", theta_true, "\n")
cat("theta_trueU   =", est_trueU, "\n")
cat("theta_param   =", est_param, "\n")
cat("theta_ecdf    =", est_ecdf, "\n")
```
